# Порядок решения задачи
## 1. Чтение информации из файла
Для чтения файла использовалась библиотека `pandas`
При чтения выяснилось что файл большой, что оперативной памяти программы не хватает. Поэтому было принято решение писать в `google colab`, так как это самый простой решения данной проблеиы, а не искать способы оптимизации, потому что последующие пункты требуют больших ресурсов компьютера
```
file_path = 'DataSet.ods'
data = pd.read_excel(file_path, engine='odf')
```
## 2. Фильтрация данных
Реализованно с помощью встроенных функцию python
```
filtered_data = data[(data['CWE-ID'] != 'NVD-CWE-Other') & (data['CVSS-V3'] != 'None') & (data['CVSS-V3'].notna())] 

```
## 3. Кластеризация
### a. Векторизация 
### 1. **Bag of Words (BoW)**
- **Описание**: Простой метод, который представляет текст как "мешок" слов, игнорируя порядок слов. Каждое слово представлено векторами, где количество каждого слова в тексте используется как значение.
- **Плюсы**:
- Простота и легкость в реализации.
- Хорошо работает для многих базовых задач классификации.
- **Минусы**:
- Игнорирует порядок и структуру слов.
- Может создавать большие разреженные матрицы, особенно с большим словарем.
- Не учитывает семантику слов (например, синонимы).

### 2. **TF-IDF (Term Frequency-Inverse Document Frequency)**
- **Описание**: Учитывает как частоту слова в документе (TF), так и как часто это слово встречается во всех документах (IDF). Это помогает уменьшить вес часто встречающихся слов, которые могут не иметь значимой информации.
- **Плюсы**:
- Учитывает важность слов в текстах, что может улучшить качество классификации.
- Уменьшает вес общих слов (стоп-слов).
- **Минусы**:
- Также не учитывает порядок слов.
- Могут возникать проблемы с разреженными матрицами.
### 3. **FastText**
- **Описание**: Модификация Word2Vec от Facebook, которая учитывает подслова (n-граммы), что позволяет лучше справляться с морфологией языков.
- **Плюсы**:
- Более эффективен для обработки словоформ и редких слов.
- Может генерировать векторы для слов, которые не встречались в обучающем наборе.
- **Минусы**:
- Сложнее в реализации по сравнению с базовыми методами.
- Все еще может зависеть от качества обучающего материала.

Будем использовать TF-IDF так как он отражает важность слова, что нам и надо (от модели FastText отказались так как надо обучать ее, так как она нуждается в коректеровке порамметров что долго)

```
descriptions = filtered_data['DESCRIPTION'].astype(str)
vectorizer = TfidfVectorizer(stop_words='english',max_features=1000)
tfidf_matrix = vectorizer.fit_transform(descriptions)
```
### b. Выбор алгоритма кластеризации
1. **K-Means**:
- **Преимущества**: Простой в реализации, хорошо работает при большом количестве данных, быстрое выполнение.
- **Недостатки**: Требует заранее задать количество кластеров, чувствителен к выбросам и начальным условиям, не подходит для кластеров различной формы.

2. **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise):
- **Преимущества**: Хорошо справляется с кластерами произвольной формы, находит выбросы.
- **Недостатки**: Необходимость подбора параметров (плотность), может плохо работать с кластерами разной плотности.

DBSCAN:
```
dbscan = DBSCAN(eps=1.3, min_samples=10, metric='euclidean')
clusters = dbscan.fit_predict(tfidf_matrix)
```

KMean
```
# Определение числа кластеров
n_clusters = 100 
kmeans = KMeans(n_clusters=n_clusters, random_state=394)

# Применение KMeans для кластеризации
kmeans.fit(tfidf_matrix)
```
Этими двумя методами будем кластеризировать. Далее сравним резултаты.
  Параметры для DBSACAN выбраны мотодом локтя
```
k = 10  # min_samples
neigh = NearestNeighbors(n_neighbors=k)
nbrs = neigh.fit(tfidf_matrix)
distances, indices = nbrs.kneighbors(tfidf_matrix)

# Для каждого точки возьмем k-ое расстояние
distances = np.sort(distances[:, k-1], axis=0)
plt.plot(distances)
plt.title("k-distance graph")
plt.xlabel("Points sorted by distance")
plt.ylabel(f"{k}-th nearest neighbor distance")
plt.show()
```
### 4. Анализ кластеров по CWE-ID и SEVERITY
Преведены на грфиках 
