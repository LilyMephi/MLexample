# Порядок решения задачи
## 1. Чтение информации из файла
Для чтения файла использовалась библиотека `pandas`
При чтения выяснилось что файл большой, что оперативной памяти программы не хватает. Поэтому было принято решение писать в `google colab`, так как это самый простой решения данной проблеиы, а не искать способы оптимизации, потому что последующие пункты требуют больших ресурсов компьютера
```
file_path = 'DataSet.ods'
data = pd.read_excel(file_path, engine='odf')
```
## 2. Фильтрация данных
Реализованно с помощью встроенных функцию python
```
descriptions = data['DESCRIPTION'].astype(str)
```
## 3. Кластеризация
### a. Векторизация 
### 1. **Bag of Words (BoW)**
- **Описание**: Простой метод, который представляет текст как "мешок" слов, игнорируя порядок слов. Каждое слово представлено векторами, где количество каждого слова в тексте используется как значение.
- **Плюсы**:
- Простота и легкость в реализации.
- Хорошо работает для многих базовых задач классификации.
- **Минусы**:
- Игнорирует порядок и структуру слов.
- Может создавать большие разреженные матрицы, особенно с большим словарем.
- Не учитывает семантику слов (например, синонимы).

### 2. **TF-IDF (Term Frequency-Inverse Document Frequency)**
- **Описание**: Учитывает как частоту слова в документе (TF), так и как часто это слово встречается во всех документах (IDF). Это помогает уменьшить вес часто встречающихся слов, которые могут не иметь значимой информации.
- **Плюсы**:
- Учитывает важность слов в текстах, что может улучшить качество классификации.
- Уменьшает вес общих слов (стоп-слов).
- **Минусы**:
- Также не учитывает порядок слов.
- Могут возникать проблемы с разреженными матрицами.
### 3. **FastText**
- **Описание**: Модификация Word2Vec от Facebook, которая учитывает подслова (n-граммы), что позволяет лучше справляться с морфологией языков.
- **Плюсы**:
- Более эффективен для обработки словоформ и редких слов.
- Может генерировать векторы для слов, которые не встречались в обучающем наборе.
- **Минусы**:
- Сложнее в реализации по сравнению с базовыми методами.
- Все еще может зависеть от качества обучающего материала.

Будем использовать TF-IDF так как он отражает важность слова, что нам и надо (от модели FastText отказались так как надо обучать ее, так как она нуждается в коректеровке порамметров что долго)

```
vectorizer = TfidfVectorizer(stop_words='english',max_features=1000)
tfidf_matrix = vectorizer.fit_transform(descriptions)
```
### b. Выбор алгоритма кластеризации
1. **K-Means**:
- **Преимущества**: Простой в реализации, хорошо работает при большом количестве данных, быстрое выполнение.
- **Недостатки**: Требует заранее задать количество кластеров, чувствителен к выбросам и начальным условиям, не подходит для кластеров различной формы.

2. **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise):
- **Преимущества**: Хорошо справляется с кластерами произвольной формы, находит выбросы.
- **Недостатки**: Необходимость подбора параметров (плотность), может плохо работать с кластерами разной плотности.

Этими двумя методами будем кластеризировать. Далее сравним резултаты
Параметры для DBSACAN выбпаны мотодом локтя
### 4. Анализ кластеров по CWE-ID и SEVERITY
Преведены на грфиках 
